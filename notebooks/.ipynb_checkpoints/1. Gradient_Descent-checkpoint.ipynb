{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Regression of Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Equation\n",
    "> y = wx + b \n",
    "\n",
    "> w is slope, b is y-intercept\n",
    "\n",
    "### Error Function - Mean Squared Error\n",
    "should be aware that there are lots of error function for different tasks, **mean squared error** function is the one\n",
    "\n",
    "we did not choose **sum squared error**, because if using a lot of data, summing up all the weight steps can lead to really large updates that make the gradient descent diverge.\n",
    "\n",
    "for linear regression\n",
    "\n",
    "<img src=\"asset/mean_squared_error_img.png\",width=200,height=200, style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute mean squared error\n",
    "def compute_error_for_line_given_points(b, w, points):\n",
    "    totalError = 0;\n",
    "    n = len(points)\n",
    "    for i in range(n):\n",
    "        x = points[i,0]\n",
    "        y = points[i,1]\n",
    "        y_prime = (w*x+b)\n",
    "        totalError += (y-y_prime)**2\n",
    "    return totalError / float(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "there are 3 variant of gradient descent algorithm: **Batch Gradient Descent**,  **Stochastic Gradient Descent**,  **Mini-batch Gradient Descent**.\n",
    "\n",
    "for details you can refer to http://sebastianruder.com/optimizing-gradient-descent/index.html#batchgradientdescent\n",
    "\n",
    ">In this notebook, we will implement 2 variants: **Batch Gradient Descent** and **Stochastic Gradient Descent**.\n",
    "\n",
    "Using [chain rule](https://en.wikipedia.org/wiki/Chain_rule) to calculate [partial derivative](https://en.wikipedia.org/wiki/Partial_derivative)\n",
    "\n",
    "<img src=\"asset/gradient_descent_demo.png\",width=800,height=800, style=\"float: left;\">\n",
    "<img src=\"asset/gradient_descent_linear_equation_2.png\",width=350,height=350, style=\"float: left;\">\n",
    "<img src=\"asset/gradient_descent_linear_equation_1.png\",width=600,height=600, style=\"float: left;\">\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient descent could lead the weights into a local minimum\n",
    "\n",
    "There are methods to avoid this, such as using [momentum](http://sebastianruder.com/optimizing-gradient-descent/index.html#momentum)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Batch Gradient Descent VS Stochastic Gradient Descent** ?\n",
    "Batch gradient descent computes the gradient using the whole dataset. \n",
    "\n",
    "Stochastic gradient descent (SGD) computes the gradient using a single sample.\n",
    "\n",
    "Batch gradient descent **performs redundant computations for large datasets**, as it recomputes gradients for similar examples before each parameter update. SGD does away with this redundancy by performing one update at a time. It is therefore usually much **faster** and can also be used to learn online. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> in code, batch gradient descent is like below:\n",
    "\n",
    "<img src=\"asset/batch_gradient_descent.png\",width=550,height=550, style=\"float: left;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Batch gradient descent\n",
    "def step_batch_gradient(b_current, w_current, points, learningRate):\n",
    "    w_gradient = 0\n",
    "    b_gradient = 0\n",
    "    n = len(points)\n",
    "    \n",
    "    # compute gradient of all points with partial derivative\n",
    "    for i in range(n):\n",
    "        x = points[i,0]\n",
    "        y = points[i,1]\n",
    "        w_gradient += (2/n)*(-x)*(y-(w_current*x+b_current))\n",
    "        b_gradient += (2/n)*(-1)*(y-(w_current*x+b_current))\n",
    "    \n",
    "    w_new = w_current - learningRate*w_gradient\n",
    "    b_new = b_current - learningRate*b_gradient\n",
    "    \n",
    "    return [w_new, b_new]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> in code, stochastic gradient descent is like below:\n",
    "\n",
    "<img src=\"asset/stochastic_gradient descent.png\",width=550,height=550, style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stochastic gradient descent\n",
    "def step_stochatic_gradient(b_current, w_current, points, learningRate):\n",
    "    w_gradient = 0\n",
    "    b_gradient = 0\n",
    "    w_new = w_current\n",
    "    b_new = b_current\n",
    "    n = len(points)\n",
    "    \n",
    "    # compute gradient applied on each point with partial derivative\n",
    "    for i in range(n):\n",
    "        x = points[i,0]\n",
    "        y = points[i,1]\n",
    "        w_gradient += (2/n)*(-x)*(y-(w_new*x+b_new))\n",
    "        b_gradient += (2/n)*(-1)*(y-(w_new*x+b_new))\n",
    "        w_new = w_new - learningRate*w_gradient\n",
    "        b_new = w_new - learningRate*b_gradient\n",
    "    \n",
    "    return [w_new, b_new]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training process \n",
    "it is to minize the error function and get the best fit line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training process to minize error function with batch gradient descent\n",
    "def batch_gradient_descent_runner(points, starting_b, starting_w, learning_rate, num_iterations, loss_values):\n",
    "    b = starting_b\n",
    "    w = starting_w\n",
    "    for i in range(num_iterations):\n",
    "        b, w = step_batch_gradient(b, w, np.array(points), learning_rate)\n",
    "        loss_values.append(compute_error_for_line_given_points(b, w, points))\n",
    "    return [b, w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training process to minize error function with stochastic gradient descent\n",
    "def stochastic_gradient_descent_runner(points, starting_b, starting_w, learning_rate, num_iterations, loss_values):\n",
    "    b = starting_b\n",
    "    w = starting_w\n",
    "    for i in range(num_iterations):\n",
    "        b, w = step_stochatic_gradient(b, w, np.array(points), learning_rate)\n",
    "        loss_values.append(compute_error_for_line_given_points(b, w, points))\n",
    "    return [b, w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch gradient descent at b = 0, m = 0, error = 5565.107834483211\n",
      "Running...\n",
      "After 5 iterations b = 1.2852728334450387, m = 1.103095380059017, error = 419.0767071396876\n",
      "\n",
      "Starting stochastic gradient descent at b = 0, m = 0, error = 5565.107834483211\n",
      "Running...\n",
      "After 5 iterations b = 1.6167305128754565, m = 1.6168551111935543, error = 183.05467595750463\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEKCAYAAADq59mMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJGIKDxERiZjCQ0REIvb/A5Pp7ZZ6R/8MAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ac005f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run():\n",
    "    points = np.genfromtxt(\"data/data_regression.csv\", delimiter=\",\")\n",
    "    \n",
    "    # using batch gradient descent\n",
    "    loss_values = []\n",
    "    learning_rate = 0.0001\n",
    "    initial_b = 0 # initial y-intercept guess\n",
    "    initial_w = 0 # initial slope guess\n",
    "    num_iterations = 5\n",
    "    print(\"Starting batch gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_w, compute_error_for_line_given_points(initial_b, initial_w, points)))\n",
    "    print(\"Running...\")\n",
    "    [b, w] = batch_gradient_descent_runner(points, initial_b, initial_w, learning_rate, num_iterations, loss_values)\n",
    "    print(\"After {0} iterations b = {1}, m = {2}, error = {3}\".format(num_iterations, b, w, compute_error_for_line_given_points(b, w, points)))\n",
    "    plt.plot(loss_values, label=\"batch\")\n",
    "    \n",
    "    print(\"\")\n",
    "    # using stochastic gradient descent\n",
    "    loss_values = []\n",
    "    learning_rate = 0.00001\n",
    "    initial_b = 0 # initial y-intercept guess\n",
    "    initial_w = 0 # initial slope guess\n",
    "    num_iterations = 5\n",
    "    print(\"Starting stochastic gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_w, compute_error_for_line_given_points(initial_b, initial_w, points)))\n",
    "    print(\"Running...\")\n",
    "    [b, w] = stochastic_gradient_descent_runner(points, initial_b, initial_w, learning_rate, num_iterations, loss_values)\n",
    "    print(\"After {0} iterations b = {1}, m = {2}, error = {3}\".format(num_iterations, b, w, compute_error_for_line_given_points(b, w, points)))\n",
    "    plt.plot(loss_values, label=\"stochastic\")\n",
    "    \n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.ylabel(\"error\")\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sigmoid Classification of Gradient Descent with simplest NN\n",
    "\n",
    "it is better to using cross-entropy error function for NN classification problem. However, this notebook is just for demo how Gradient Descent implement, so we just choose MSE for demo. For details of cross-entropy error over mean squared error, refer to below:\n",
    "\n",
    "https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_prep module is data_prep.py\n",
    "from data_prep import *\n",
    "features, targets, features_test, targets_test = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>rank_1</th>\n",
       "      <th>rank_2</th>\n",
       "      <th>rank_3</th>\n",
       "      <th>rank_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>-0.066657</td>\n",
       "      <td>0.289305</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>0.625884</td>\n",
       "      <td>1.445476</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.837832</td>\n",
       "      <td>1.603135</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>1.318426</td>\n",
       "      <td>-0.131120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>-0.066657</td>\n",
       "      <td>-1.208461</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          gre       gpa  rank_1  rank_2  rank_3  rank_4\n",
       "209 -0.066657  0.289305       0       1       0       0\n",
       "280  0.625884  1.445476       0       1       0       0\n",
       "33   1.837832  1.603135       0       0       1       0\n",
       "210  1.318426 -0.131120       0       0       0       1\n",
       "93  -0.066657 -1.208461       0       1       0       0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209    0\n",
       "280    0\n",
       "33     1\n",
       "210    0\n",
       "93     0\n",
       "Name: admit, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"asset/sigmoid_gradient.png\",width=550,height=550, style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the sigmoid function for activations\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Derivative of the sigmoid function\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loss function using mean squared error\n",
    "def batch_gradient_descent(weights, features, targets, learnrate):\n",
    "    del_w = np.zeros(weights.shape)\n",
    "    # Loop through all records, x is the input, y is the target\n",
    "    for x, y in zip(features.values, targets):\n",
    "        # Calculate the output\n",
    "        output = sigmoid(np.dot(x,weights))\n",
    "        # Calculate change in weights. we can also call sigmoid_prime instead of \"output * (1-output)\"\n",
    "        # Simultaneously calculate all features in one data \n",
    "        del_w += (2/n_records) * (y-output) * output * (1-output) * x\n",
    "\n",
    "        # Update weights\n",
    "    weights += learnrate * del_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.255890746086\n",
      "Train loss:  0.250608169742\n",
      "Train loss:  0.246132059349\n",
      "Train loss:  0.242295804234\n",
      "Train loss:  0.238965365732\n",
      "Train loss:  0.236037208065\n",
      "Train loss:  0.233432917265\n",
      "Train loss:  0.231093477031\n",
      "Train loss:  0.228974350895\n",
      "Train loss:  0.227041630595\n"
     ]
    }
   ],
   "source": [
    "n_records, n_features = features.shape\n",
    "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "# Neural Network hyperparameters\n",
    "epochs = 1000\n",
    "learnrate = 0.01\n",
    "\n",
    "def train():\n",
    "    for e in range(epochs):\n",
    "        batch_gradient_descent(weights, features, targets, learnrate)\n",
    "        # Printing out the mean square error on the training set\n",
    "        if e % (epochs / 10) == 0:\n",
    "            out = sigmoid(np.dot(features, weights))\n",
    "            loss = np.mean((out - targets) ** 2)           \n",
    "            print(\"Train loss: \", loss)\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy: 0.725\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy on test data\n",
    "tes_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = tes_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
