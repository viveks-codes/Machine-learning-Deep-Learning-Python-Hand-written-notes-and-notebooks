{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "This notebook will show you the underlying implementation of Backpropagation for a simple neural network with 3 layers(input_layer, hidden_layer, output_layer).\n",
    "\n",
    "If you have any suggestions, welcome to contact [hushenglang@gmail.com](), or pull request on github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data_prep module is data_prep.py\n",
    "from data_prep import *\n",
    "features, targets, features_test, targets_test = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>rank_1</th>\n",
       "      <th>rank_2</th>\n",
       "      <th>rank_3</th>\n",
       "      <th>rank_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>-0.066657</td>\n",
       "      <td>0.289305</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>0.625884</td>\n",
       "      <td>1.445476</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.837832</td>\n",
       "      <td>1.603135</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>1.318426</td>\n",
       "      <td>-0.131120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>-0.066657</td>\n",
       "      <td>-1.208461</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          gre       gpa  rank_1  rank_2  rank_3  rank_4\n",
       "209 -0.066657  0.289305       0       1       0       0\n",
       "280  0.625884  1.445476       0       1       0       0\n",
       "33   1.837832  1.603135       0       0       1       0\n",
       "210  1.318426 -0.131120       0       0       0       1\n",
       "93  -0.066657 -1.208461       0       1       0       0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209    0\n",
       "280    0\n",
       "33     1\n",
       "210    0\n",
       "93     0\n",
       "Name: admit, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation Implementation\n",
    "\n",
    "loss function used in this implementaion is **MSE**\n",
    "\n",
    "for sure, you can change to use other loss function like **cross-entropy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"asset/back_propagation1.png\",width=500,height=500, style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"asset/back_propagation2.png\",width=200,height=200, style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"asset/back_propagation3.png\",width=200,height=200, style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**chain rule** to backpropagate to calculate **w**\n",
    "\n",
    "<img src=\"asset/back_propagation4.png\",width=600,height=600, style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation is a leaky abstraction.\n",
    "\n",
    "It may cause Vanishing gradients on sigmoids, Dying ReLUs and Exploding gradients in RNNs.\n",
    "\n",
    "please refer to this post for details:\n",
    "\n",
    "https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the sigmoid function for activations\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# backpropagation with batch gradient descent\n",
    "def backprop_batch_gradient_descent(weights_input_hidden, weights_hidden_output, features, targets):\n",
    "    del_w_input_hidden = np.zeros(weights_input_hidden.shape)\n",
    "    del_w_hidden_output = np.zeros(weights_hidden_output.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        ## Forward pass ##\n",
    "        hidden_input = np.dot(x, weights_input_hidden)\n",
    "        hidden_activations = sigmoid(hidden_input)\n",
    "        output = sigmoid(np.dot(hidden_activations, weights_hidden_output))\n",
    "\n",
    "        ## Backward pass ##\n",
    "        # Calculate error gradient in output unit\n",
    "        output_error_gradient = (y-output) * output * (1-output)\n",
    "\n",
    "        # propagate errors to hidden layer hidden\n",
    "        hidden_error_gradient = weights_hidden_output * output_error_gradient * hidden_activations * (1-hidden_activations)\n",
    "\n",
    "        # Update the change in weights\n",
    "        del_w_hidden_output +=  output_error_gradient * hidden_activations\n",
    "        del_w_input_hidden +=  hidden_error_gradient * x[:, None]\n",
    "\n",
    "    # Update weights\n",
    "    weights_input_hidden += learnrate * del_w_input_hidden / n_records\n",
    "    weights_hidden_output += learnrate * del_w_hidden_output / n_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_hidden = 20  # number of hidden units\n",
    "epochs = 1000\n",
    "learnrate = 0.05\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "# Initialize weights\n",
    "weights_input_hidden = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                        size=(n_features, n_hidden))\n",
    "weights_hidden_output = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                         size=n_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.228492837836\n",
      "Train loss:  0.220652879861\n",
      "Train loss:  0.217154284272\n",
      "Train loss:  0.214766019789\n",
      "Train loss:  0.212850794833\n",
      "Train loss:  0.21126659987\n",
      "Train loss:  0.209941781907\n",
      "Train loss:  0.208823298017\n",
      "Train loss:  0.207869565384\n",
      "Train loss:  0.207047917328\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    for e in range(epochs):\n",
    "        backprop_batch_gradient_descent(weights_input_hidden, weights_hidden_output, features, targets)\n",
    "        # Printing out the mean square error on the training set\n",
    "        if e % (epochs / 10) == 0:\n",
    "            hidden_activations = sigmoid(np.dot(features, weights_input_hidden))\n",
    "            out = sigmoid(np.dot(hidden_activations,\n",
    "                                 weights_hidden_output))\n",
    "            loss = np.mean((out - targets) ** 2)         \n",
    "            print(\"Train loss: \", loss)\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy: 0.725\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy on test data\n",
    "hidden = sigmoid(np.dot(features_test, weights_input_hidden))\n",
    "out = sigmoid(np.dot(hidden, weights_hidden_output))\n",
    "predictions = out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading\n",
    "Understand backprop:\n",
    "\n",
    "https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b\n",
    "\n",
    "https://www.youtube.com/watch?v=59Hbtz7XgjM"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
