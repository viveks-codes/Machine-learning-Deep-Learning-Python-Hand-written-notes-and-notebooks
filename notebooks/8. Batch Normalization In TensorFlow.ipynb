{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits of Batch Normalization\n",
    "\n",
    "Batch normalization optimizes network training. It has been shown to have several benefits:\n",
    "\n",
    "1. ** Networks train faster** – Each training iteration will actually be slower because of the extra calculations during the forward pass and the additional hyperparameters to train during back propagation. However, it should converge much more quickly, so training should be faster overall.\n",
    "2. ** Allows higher learning rates** – Gradient descent usually requires small learning rates for the network to converge. And as networks get deeper, their gradients get smaller during back propagation so they require even more iterations. Using batch normalization allows us to use much higher learning rates, which further increases the speed at which networks train.\n",
    "3. ** Makes weights easier to initialize** – Weight initialization can be difficult, and it's even more difficult when creating deeper networks. Batch normalization seems to allow us to be much less careful about choosing our initial starting weights.\n",
    "4. ** Makes more activation functions viable** – Some activation functions do not work well in some situations. Sigmoids lose their gradient pretty quickly, which means they can't be used in deep networks. And ReLUs often die out during training, where they stop learning completely, so we need to be careful about the range of values fed into them. Because batch normalization regulates the values going into each activation function, non-linearlities that don't seem to work well in deep networks actually become viable again.\n",
    "5. ** Simplifies the creation of deeper networks** – Because of the first 4 items listed above, it is easier to build and faster to train deeper neural networks when using batch normalization. And it's been shown that deeper networks generally produce better results, so that's great.\n",
    "6. ** Provides a bit of regularlization** – Batch normalization adds a little noise to your network. In some cases, such as in Inception modules, batch normalization has been shown to work as well as dropout. But in general, consider batch normalization as a bit of extra regularization, possibly allowing you to reduce some of the dropout you might add to a network.\n",
    "7. ** May give better results overall** – Some tests seem to show batch normalization actually improves the training results. However, it's really an optimization to help train faster, so you shouldn't think of it as a way to make your network better. But since it lets you train networks faster, that means you can iterate over more designs more quickly. It also lets you build deeper networks, which are usually better. So when you factor in everything, you're probably going to end up with better results if you build your networks with batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Batch Normalization in TensorFlow\n",
    "\n",
    "\n",
    "This notebook includes two versions of the network that you can edit. The first uses higher level functions from the tf.layers package. The second is the same network, but uses only lower level functions in the tf.nn package.\n",
    "1. Batch Normalization with **tf.layers.batch_normalization**\n",
    "2. Batch Normalization with **tf.nn.batch_normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True, reshape=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvNet without Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0: Validation loss: 0.69066, Validation accuracy: 0.08680\n",
      "Batch: 25: Training loss: 0.33259, Training accuracy: 0.09375\n",
      "Batch: 50: Training loss: 0.32674, Training accuracy: 0.14062\n",
      "Batch: 75: Training loss: 0.32589, Training accuracy: 0.06250\n",
      "Batch: 100: Validation loss: 0.32535, Validation accuracy: 0.10700\n",
      "Batch: 125: Training loss: 0.32599, Training accuracy: 0.03125\n",
      "Batch: 150: Training loss: 0.32564, Training accuracy: 0.10938\n",
      "Batch: 175: Training loss: 0.32308, Training accuracy: 0.21875\n",
      "Batch: 200: Validation loss: 0.32583, Validation accuracy: 0.09860\n",
      "Batch: 225: Training loss: 0.32485, Training accuracy: 0.15625\n",
      "Batch: 250: Training loss: 0.32630, Training accuracy: 0.07812\n",
      "Batch: 275: Training loss: 0.32292, Training accuracy: 0.12500\n",
      "Batch: 300: Validation loss: 0.32587, Validation accuracy: 0.09240\n",
      "Batch: 325: Training loss: 0.32248, Training accuracy: 0.15625\n",
      "Batch: 350: Training loss: 0.32299, Training accuracy: 0.12500\n",
      "Batch: 375: Training loss: 0.32621, Training accuracy: 0.09375\n",
      "Batch: 400: Validation loss: 0.32497, Validation accuracy: 0.11260\n",
      "Batch: 425: Training loss: 0.32553, Training accuracy: 0.09375\n",
      "Batch: 450: Training loss: 0.32698, Training accuracy: 0.10938\n",
      "Batch: 475: Training loss: 0.32659, Training accuracy: 0.04688\n",
      "Batch: 500: Validation loss: 0.32516, Validation accuracy: 0.11000\n",
      "Batch: 525: Training loss: 0.32881, Training accuracy: 0.01562\n",
      "Batch: 550: Training loss: 0.32458, Training accuracy: 0.09375\n",
      "Batch: 575: Training loss: 0.32201, Training accuracy: 0.20312\n",
      "Batch: 600: Validation loss: 0.32552, Validation accuracy: 0.11260\n",
      "Batch: 625: Training loss: 0.32473, Training accuracy: 0.14062\n",
      "Batch: 650: Training loss: 0.32878, Training accuracy: 0.03125\n",
      "Batch: 675: Training loss: 0.32541, Training accuracy: 0.12500\n",
      "Batch: 700: Validation loss: 0.32508, Validation accuracy: 0.09860\n",
      "Batch: 725: Training loss: 0.32509, Training accuracy: 0.09375\n",
      "Batch: 750: Training loss: 0.32528, Training accuracy: 0.14062\n",
      "Batch: 775: Training loss: 0.32595, Training accuracy: 0.12500\n",
      "Final validation accuracy: 0.11000\n",
      "Final test accuracy: 0.10280\n",
      "Accuracy on 100 samples: 0.15\n"
     ]
    }
   ],
   "source": [
    "def fully_connected(prev_layer, num_units):\n",
    "    layer = tf.layers.dense(prev_layer, num_units, activation=tf.nn.relu)\n",
    "    return layer\n",
    "\n",
    "def conv_layer(prev_layer, layer_depth):\n",
    "    strides = 2 if layer_depth % 3 == 0 else 1\n",
    "    conv_layer = tf.layers.conv2d(prev_layer, layer_depth*4, 3, strides, 'same', activation=tf.nn.relu)\n",
    "    return conv_layer\n",
    "\n",
    "def train(num_batches, batch_size, learning_rate):\n",
    "    # Build placeholders for the input samples and labels \n",
    "    inputs = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    labels = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    # Feed the inputs into a series of 20 convolutional layers \n",
    "    layer = inputs\n",
    "    for layer_i in range(1, 20):\n",
    "        layer = conv_layer(layer, layer_i)\n",
    "\n",
    "    # Flatten the output from the convolutional layers \n",
    "    orig_shape = layer.get_shape().as_list()\n",
    "    layer = tf.reshape(layer, shape=[-1, orig_shape[1] * orig_shape[2] * orig_shape[3]])\n",
    "\n",
    "    # Add one fully connected layer\n",
    "    layer = fully_connected(layer, 100)\n",
    "\n",
    "    # Create the output layer with 1 node for each \n",
    "    logits = tf.layers.dense(layer, 10)\n",
    "    \n",
    "    # Define \n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    \n",
    "    train_opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # Train and test the network\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for batch_i in range(num_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # train this batch\n",
    "            sess.run(train_opt, {inputs: batch_xs, \n",
    "                                 labels: batch_ys})\n",
    "            \n",
    "            # Periodically check the validation or training loss and accuracy\n",
    "            if batch_i % 100 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: mnist.validation.images,\n",
    "                                                              labels: mnist.validation.labels})\n",
    "                print('Batch: {:>2}: Validation loss: {:>3.5f}, Validation accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "            elif batch_i % 25 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: batch_xs, labels: batch_ys})\n",
    "                print('Batch: {:>2}: Training loss: {:>3.5f}, Training accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "\n",
    "        # At the end, score the final accuracy for both the validation and test sets\n",
    "        acc = sess.run(accuracy, {inputs: mnist.validation.images,\n",
    "                                  labels: mnist.validation.labels})\n",
    "        print('Final validation accuracy: {:>3.5f}'.format(acc))\n",
    "        acc = sess.run(accuracy, {inputs: mnist.test.images,\n",
    "                                  labels: mnist.test.labels})\n",
    "        print('Final test accuracy: {:>3.5f}'.format(acc))\n",
    "        \n",
    "        # Score the first 100 test images individually, just to make sure batch normalization really worked\n",
    "        correct = 0\n",
    "        for i in range(100):\n",
    "            correct += sess.run(accuracy,feed_dict={inputs: [mnist.test.images[i]],\n",
    "                                                    labels: [mnist.test.labels[i]]})\n",
    "\n",
    "        print(\"Accuracy on 100 samples:\", correct/100)\n",
    "\n",
    "\n",
    "num_batches = 800\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    train(num_batches, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add batch normalization using tf.layers.batch_normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_connected(prev_layer, num_units, is_training):\n",
    "    layer = tf.layers.dense(prev_layer, num_units, use_bias=False, activation=None)\n",
    "    layer = tf.layers.batch_normalization(layer, training=is_training)\n",
    "    layer = tf.nn.relu(layer)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove bias, since batch normaliazation has scale and shift, but it is also ok to add bias.\n",
    "# add batch_normalization before activation function, but it is also ok to add batch_normalization after activation function.\n",
    "def conv_layer(prev_layer, layer_depth, is_training):\n",
    "    strides = 2 if layer_depth % 3 == 0 else 1\n",
    "    conv_layer = tf.layers.conv2d(prev_layer, layer_depth*4, 3, strides, 'same', use_bias=False, activation=None)\n",
    "    conv_layer = tf.layers.batch_normalization(conv_layer, training=is_training)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "\n",
    "    return conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0: Validation loss: 0.69149, Validation accuracy: 0.10020\n",
      "Batch: 25: Training loss: 0.59560, Training accuracy: 0.15625\n",
      "Batch: 50: Training loss: 0.49104, Training accuracy: 0.09375\n",
      "Batch: 75: Training loss: 0.41474, Training accuracy: 0.09375\n",
      "Batch: 100: Validation loss: 0.38543, Validation accuracy: 0.10700\n",
      "Batch: 125: Training loss: 0.36123, Training accuracy: 0.07812\n",
      "Batch: 150: Training loss: 0.34840, Training accuracy: 0.20312\n",
      "Batch: 175: Training loss: 0.32972, Training accuracy: 0.23438\n",
      "Batch: 200: Validation loss: 0.24854, Validation accuracy: 0.46860\n",
      "Batch: 225: Training loss: 0.21122, Training accuracy: 0.60938\n",
      "Batch: 250: Training loss: 0.12089, Training accuracy: 0.79688\n",
      "Batch: 275: Training loss: 0.07979, Training accuracy: 0.85938\n",
      "Batch: 300: Validation loss: 0.08199, Validation accuracy: 0.86280\n",
      "Batch: 325: Training loss: 0.01968, Training accuracy: 0.96875\n",
      "Batch: 350: Training loss: 0.01263, Training accuracy: 0.98438\n",
      "Batch: 375: Training loss: 0.04903, Training accuracy: 0.92188\n",
      "Batch: 400: Validation loss: 0.05864, Validation accuracy: 0.91520\n",
      "Batch: 425: Training loss: 0.02850, Training accuracy: 0.95312\n",
      "Batch: 450: Training loss: 0.02616, Training accuracy: 0.93750\n",
      "Batch: 475: Training loss: 0.03823, Training accuracy: 0.93750\n",
      "Batch: 500: Validation loss: 0.04525, Validation accuracy: 0.93400\n",
      "Batch: 525: Training loss: 0.05739, Training accuracy: 0.92188\n",
      "Batch: 550: Training loss: 0.03585, Training accuracy: 0.95312\n",
      "Batch: 575: Training loss: 0.01531, Training accuracy: 0.98438\n",
      "Batch: 600: Validation loss: 0.03501, Validation accuracy: 0.95280\n",
      "Batch: 625: Training loss: 0.05373, Training accuracy: 0.89062\n",
      "Batch: 650: Training loss: 0.03490, Training accuracy: 0.92188\n",
      "Batch: 675: Training loss: 0.04616, Training accuracy: 0.90625\n",
      "Batch: 700: Validation loss: 0.03187, Validation accuracy: 0.95420\n",
      "Batch: 725: Training loss: 0.06575, Training accuracy: 0.95312\n",
      "Batch: 750: Training loss: 0.01018, Training accuracy: 0.98438\n",
      "Batch: 775: Training loss: 0.01752, Training accuracy: 0.96875\n",
      "Final validation accuracy: 0.95840\n",
      "Final test accuracy: 0.96360\n",
      "Accuracy on 100 samples: 0.96\n"
     ]
    }
   ],
   "source": [
    "def train(num_batches, batch_size, learning_rate):\n",
    "    # Build placeholders for the input samples and labels \n",
    "    inputs = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    labels = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "    # Add placeholder to indicate whether or not we're training the model\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "    # Feed the inputs into a series of 20 convolutional layers \n",
    "    layer = inputs\n",
    "    for layer_i in range(1, 20):\n",
    "        layer = conv_layer(layer, layer_i, is_training)\n",
    "\n",
    "    # Flatten the output from the convolutional layers \n",
    "    orig_shape = layer.get_shape().as_list()\n",
    "    layer = tf.reshape(layer, shape=[-1, orig_shape[1] * orig_shape[2] * orig_shape[3]])\n",
    "\n",
    "    # Add one fully connected layer\n",
    "    layer = fully_connected(layer, 100, is_training)\n",
    "\n",
    "    # Create the output layer with 1 node for each \n",
    "    logits = tf.layers.dense(layer, 10)\n",
    "    \n",
    "    # Define loss and training operations\n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    \n",
    "    # Tell TensorFlow to update the population statistics while training\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        train_opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "    \n",
    "    # Create operations to test accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # Train and test the network\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for batch_i in range(num_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # train this batch\n",
    "            sess.run(train_opt, {inputs: batch_xs, labels: batch_ys, is_training: True})\n",
    "            \n",
    "            # Periodically check the validation or training loss and accuracy\n",
    "            if batch_i % 100 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: mnist.validation.images,\n",
    "                                                              labels: mnist.validation.labels,\n",
    "                                                              is_training: False})\n",
    "                print('Batch: {:>2}: Validation loss: {:>3.5f}, Validation accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "            elif batch_i % 25 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: batch_xs, labels: batch_ys, is_training: False})\n",
    "                print('Batch: {:>2}: Training loss: {:>3.5f}, Training accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "\n",
    "        # At the end, score the final accuracy for both the validation and test sets\n",
    "        acc = sess.run(accuracy, {inputs: mnist.validation.images,\n",
    "                                  labels: mnist.validation.labels, \n",
    "                                  is_training: False})\n",
    "        print('Final validation accuracy: {:>3.5f}'.format(acc))\n",
    "        acc = sess.run(accuracy, {inputs: mnist.test.images,\n",
    "                                  labels: mnist.test.labels,\n",
    "                                  is_training: False})\n",
    "        print('Final test accuracy: {:>3.5f}'.format(acc))\n",
    "        \n",
    "        # Score the first 100 test images individually, just to make sure batch normalization really worked\n",
    "        correct = 0\n",
    "        for i in range(100):\n",
    "            correct += sess.run(accuracy,feed_dict={inputs: [mnist.test.images[i]],\n",
    "                                                    labels: [mnist.test.labels[i]],\n",
    "                                                    is_training: False})\n",
    "\n",
    "        print(\"Accuracy on 100 samples:\", correct/100)\n",
    "\n",
    "\n",
    "num_batches = 800\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    train(num_batches, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tips: If this value is low while everything else looks good, that means you did not implement batch normalization correctly. Specifically, it means you either did not calculate the population mean and variance while training, or you are not using those values during inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Batch Normalization using tf.nn.batch_normalization\n",
    "\n",
    "Most of the time you will be able to use higher level functions exclusively, but sometimes you may want to work at a lower level. For example, if you ever want to implement a new feature – something new enough that TensorFlow does not already include a high-level implementation of it, like batch normalization in an LSTM – then you may need to know these sorts of things.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_connected(prev_layer, num_units, is_training):\n",
    "    \"\"\"\n",
    "    Create a fully connectd layer with the given layer as input and the given number of neurons.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param num_units: int\n",
    "        The size of the layer. That is, the number of units, nodes, or neurons.\n",
    "    :param is_training: bool or Tensor\n",
    "        Indicates whether or not the network is currently training, which tells the batch normalization\n",
    "        layer whether or not it should update or use its population statistics.\n",
    "    :returns Tensor\n",
    "        A new fully connected layer\n",
    "    \"\"\"\n",
    "\n",
    "    layer = tf.layers.dense(prev_layer, num_units, use_bias=False, activation=None)\n",
    "\n",
    "    gamma = tf.Variable(tf.ones([num_units]))\n",
    "    beta = tf.Variable(tf.zeros([num_units]))\n",
    "\n",
    "    pop_mean = tf.Variable(tf.zeros([num_units]), trainable=False)\n",
    "    pop_variance = tf.Variable(tf.ones([num_units]), trainable=False)\n",
    "\n",
    "    epsilon = 1e-3\n",
    "    \n",
    "    def batch_norm_training():\n",
    "        batch_mean, batch_variance = tf.nn.moments(layer, [0])\n",
    "\n",
    "        decay = 0.99\n",
    "        train_mean = tf.assign(pop_mean, pop_mean * decay + batch_mean * (1 - decay))\n",
    "        train_variance = tf.assign(pop_variance, pop_variance * decay + batch_variance * (1 - decay))\n",
    "\n",
    "        with tf.control_dependencies([train_mean, train_variance]):\n",
    "            return tf.nn.batch_normalization(layer, batch_mean, batch_variance, beta, gamma, epsilon)\n",
    " \n",
    "    def batch_norm_inference():\n",
    "        return tf.nn.batch_normalization(layer, pop_mean, pop_variance, beta, gamma, epsilon)\n",
    "\n",
    "    batch_normalized_output = tf.cond(is_training, batch_norm_training, batch_norm_inference)\n",
    "    return tf.nn.relu(batch_normalized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(prev_layer, layer_depth, is_training):\n",
    "    \"\"\"\n",
    "    Create a convolutional layer with the given layer as input.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param layer_depth: int\n",
    "        We'll set the strides and number of feature maps based on the layer's depth in the network.\n",
    "        This is *not* a good way to make a CNN, but it helps us create this example with very little code.\n",
    "    :param is_training: bool or Tensor\n",
    "        Indicates whether or not the network is currently training, which tells the batch normalization\n",
    "        layer whether or not it should update or use its population statistics.\n",
    "    :returns Tensor\n",
    "        A new convolutional layer\n",
    "    \"\"\"\n",
    "    strides = 2 if layer_depth % 3 == 0 else 1\n",
    "    \n",
    "    in_channels = prev_layer.get_shape().as_list()[3]\n",
    "    out_channels = layer_depth*4\n",
    "    \n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([3, 3, in_channels, out_channels], stddev=0.05))\n",
    "    \n",
    "    layer = tf.nn.conv2d(prev_layer, weights, strides=[1,strides, strides, 1], padding='SAME')\n",
    "\n",
    "    gamma = tf.Variable(tf.ones([out_channels]))\n",
    "    beta = tf.Variable(tf.zeros([out_channels]))\n",
    "\n",
    "    pop_mean = tf.Variable(tf.zeros([out_channels]), trainable=False)\n",
    "    pop_variance = tf.Variable(tf.ones([out_channels]), trainable=False)\n",
    "\n",
    "    epsilon = 1e-3\n",
    "    \n",
    "    def batch_norm_training():\n",
    "        # Important to use the correct dimensions here to ensure the mean and variance are calculated \n",
    "        # per feature map instead of for the entire layer\n",
    "        batch_mean, batch_variance = tf.nn.moments(layer, [0,1,2], keep_dims=False)\n",
    "\n",
    "        decay = 0.99\n",
    "        train_mean = tf.assign(pop_mean, pop_mean * decay + batch_mean * (1 - decay))\n",
    "        train_variance = tf.assign(pop_variance, pop_variance * decay + batch_variance * (1 - decay))\n",
    "\n",
    "        with tf.control_dependencies([train_mean, train_variance]):\n",
    "            return tf.nn.batch_normalization(layer, batch_mean, batch_variance, beta, gamma, epsilon)\n",
    " \n",
    "    def batch_norm_inference():\n",
    "        return tf.nn.batch_normalization(layer, pop_mean, pop_variance, beta, gamma, epsilon)\n",
    "\n",
    "    batch_normalized_output = tf.cond(is_training, batch_norm_training, batch_norm_inference)\n",
    "    return tf.nn.relu(batch_normalized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0: Validation loss: 0.69120, Validation accuracy: 0.09240\n",
      "Batch: 25: Training loss: 0.59046, Training accuracy: 0.06250\n",
      "Batch: 50: Training loss: 0.48439, Training accuracy: 0.12500\n",
      "Batch: 75: Training loss: 0.41505, Training accuracy: 0.07812\n",
      "Batch: 100: Validation loss: 0.37488, Validation accuracy: 0.09240\n",
      "Batch: 125: Training loss: 0.36008, Training accuracy: 0.07812\n",
      "Batch: 150: Training loss: 0.33396, Training accuracy: 0.17188\n",
      "Batch: 175: Training loss: 0.34036, Training accuracy: 0.10938\n",
      "Batch: 200: Validation loss: 0.34051, Validation accuracy: 0.15680\n",
      "Batch: 225: Training loss: 0.35200, Training accuracy: 0.09375\n",
      "Batch: 250: Training loss: 0.35101, Training accuracy: 0.20312\n",
      "Batch: 275: Training loss: 0.32452, Training accuracy: 0.26562\n",
      "Batch: 300: Validation loss: 0.45633, Validation accuracy: 0.14040\n",
      "Batch: 325: Training loss: 0.39842, Training accuracy: 0.18750\n",
      "Batch: 350: Training loss: 0.32794, Training accuracy: 0.42188\n",
      "Batch: 375: Training loss: 0.43578, Training accuracy: 0.35938\n",
      "Batch: 400: Validation loss: 0.30611, Validation accuracy: 0.52900\n",
      "Batch: 425: Training loss: 0.25743, Training accuracy: 0.57812\n",
      "Batch: 450: Training loss: 0.18273, Training accuracy: 0.68750\n",
      "Batch: 475: Training loss: 0.03904, Training accuracy: 0.93750\n",
      "Batch: 500: Validation loss: 0.06491, Validation accuracy: 0.90860\n",
      "Batch: 525: Training loss: 0.00707, Training accuracy: 0.96875\n",
      "Batch: 550: Training loss: 0.02128, Training accuracy: 0.95312\n",
      "Batch: 575: Training loss: 0.00947, Training accuracy: 0.98438\n",
      "Batch: 600: Validation loss: 0.03991, Validation accuracy: 0.94460\n",
      "Batch: 625: Training loss: 0.02212, Training accuracy: 0.95312\n",
      "Batch: 650: Training loss: 0.00936, Training accuracy: 0.98438\n",
      "Batch: 675: Training loss: 0.03648, Training accuracy: 0.93750\n",
      "Batch: 700: Validation loss: 0.02602, Validation accuracy: 0.96460\n",
      "Batch: 725: Training loss: 0.02674, Training accuracy: 0.95312\n",
      "Batch: 750: Training loss: 0.00158, Training accuracy: 1.00000\n",
      "Batch: 775: Training loss: 0.04857, Training accuracy: 0.93750\n",
      "Final validation accuracy: 0.97180\n",
      "Final test accuracy: 0.96870\n",
      "Accuracy on 100 samples: 0.97\n"
     ]
    }
   ],
   "source": [
    "def train(num_batches, batch_size, learning_rate):\n",
    "    # Build placeholders for the input samples and labels \n",
    "    inputs = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    labels = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "    # Add placeholder to indicate whether or not we're training the model\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "    # Feed the inputs into a series of 20 convolutional layers \n",
    "    layer = inputs\n",
    "    for layer_i in range(1, 20):\n",
    "        layer = conv_layer(layer, layer_i, is_training)\n",
    "\n",
    "    # Flatten the output from the convolutional layers \n",
    "    orig_shape = layer.get_shape().as_list()\n",
    "    layer = tf.reshape(layer, shape=[-1, orig_shape[1] * orig_shape[2] * orig_shape[3]])\n",
    "\n",
    "    # Add one fully connected layer\n",
    "    layer = fully_connected(layer, 100, is_training)\n",
    "\n",
    "    # Create the output layer with 1 node for each \n",
    "    logits = tf.layers.dense(layer, 10)\n",
    "    \n",
    "    # Define loss and training operations\n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    train_opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "    \n",
    "    # Create operations to test accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # Train and test the network\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for batch_i in range(num_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # train this batch\n",
    "            sess.run(train_opt, {inputs: batch_xs, labels: batch_ys, is_training: True})\n",
    "            \n",
    "            # Periodically check the validation or training loss and accuracy\n",
    "            if batch_i % 100 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: mnist.validation.images,\n",
    "                                                              labels: mnist.validation.labels,\n",
    "                                                              is_training: False})\n",
    "                print('Batch: {:>2}: Validation loss: {:>3.5f}, Validation accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "            elif batch_i % 25 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: batch_xs, labels: batch_ys, is_training: False})\n",
    "                print('Batch: {:>2}: Training loss: {:>3.5f}, Training accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "\n",
    "        # At the end, score the final accuracy for both the validation and test sets\n",
    "        acc = sess.run(accuracy, {inputs: mnist.validation.images,\n",
    "                                  labels: mnist.validation.labels, \n",
    "                                  is_training: False})\n",
    "        print('Final validation accuracy: {:>3.5f}'.format(acc))\n",
    "        acc = sess.run(accuracy, {inputs: mnist.test.images,\n",
    "                                  labels: mnist.test.labels,\n",
    "                                  is_training: False})\n",
    "        print('Final test accuracy: {:>3.5f}'.format(acc))\n",
    "        \n",
    "        # Score the first 100 test images individually, just to make sure batch normalization really worked\n",
    "        correct = 0\n",
    "        for i in range(100):\n",
    "            correct += sess.run(accuracy,feed_dict={inputs: [mnist.test.images[i]],\n",
    "                                                    labels: [mnist.test.labels[i]],\n",
    "                                                    is_training: False})\n",
    "\n",
    "        print(\"Accuracy on 100 samples:\", correct/100)\n",
    "\n",
    "\n",
    "num_batches = 800\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    train(num_batches, batch_size, learning_rate)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
